@article{Cornelis2003-TaxonomyReplay,
  author = {Cornelis, Frank and Georges, Andy and Christiaens, Mark and Ronsse, Michiel and Ghesquiere, Tom and De Bosschere, Koen},
  year = {2003},
  month = {01},
  pages = {},
  title = {A Taxonomy of Execution Replay Systems}
}

@article{Dionne1996-TaxonomyDebuggers,
author = {Dionne, Carl and Feeley, Marc and Desbiens, Jocelyn and Informatique, Alex},
year = {1996},
month = {09},
pages = {},
title = {A Taxonomy of Distributed Debuggers Based on Execution Replay}
}

@inproceedings{Richards2011-JavascripBenchmarks,
  author = {Richards, Gregor and Gal, Andreas and Eich, Brendan and Vitek, Jan},
  title = {Automated Construction of JavaScript Benchmarks},
  year = {2011},
  isbn = {9781450309400},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2048066.2048119},
  doi = {10.1145/2048066.2048119},
  abstract = {JavaScript is a highly dynamic language for web-based applications. Innovative implementation techniques for improving its speed and responsiveness have been developed in recent years. Industry benchmarks such as WebKit SunSpider are often cited as a measure of the efficacy of these techniques. However, recent studies have shown that these benchmarks fail to accurately represent the dynamic nature of modern JavaScript applications, and so may be poor predictors of real-world performance. Worse, they may guide the development of optimizations which are unhelpful for real applications. Our goal is to develop a tool and techniques to automate the creation of realistic and representative benchmarks from existing web applications. We propose a record-and-replay approach to capture JavaScript sessions which has sufficient fidelity to accurately recreate key characteristics of the original application, and at the same time is sufficiently flexible that a recording produced on one platform can be replayed on a different one. We describe JSBench, a flexible tool for workload capture and benchmark generation, and demonstrate its use in creating eight benchmarks based on popular sites. Using a variety of runtime metrics collected with instrumented versions of Firefox, Internet Explorer, and Safari, we show that workloads created by JSBench match the behavior of the original web applications.},
  booktitle = {Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications},
  pages = {677â€“694},
  numpages = {18},
  keywords = {reproduction, benchmarks, repetition},
  location = {Portland, Oregon, USA},
  series = {OOPSLA '11}
}